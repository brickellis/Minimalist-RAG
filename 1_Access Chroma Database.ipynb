{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBcy3WcCxnTP"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Load the chromadb created in the previous notebook using the same embedding model.\n",
        "'''\n",
        "file_name = \"history\"\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "db = Chroma(persist_directory=file_name + '_db', embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Here is an example of retrieving documents from the db directly.\n",
        "The main problems with this method is:\n",
        "1. It can take a while to read the returned docs and find the required information.\n",
        "2. The information may not be in a similar semantic form to the question, meaning that \n",
        "the embeddings won't be similar and the doc containing the required information won't be returned.\n",
        "\n",
        "The first problem will be solved below by having an LLM do the scanning for us. The second will hopefully \n",
        "be solved by research, but it can also be helped by changing how your data is structured in order to make the \n",
        "queried information more semantically predictable when attempting to query it.\n",
        "'''\n",
        "question = 'What was the name of Benjamin Franklin\\'s newspaper?'\n",
        "results = db.similarity_search_with_score(question, k=3)\n",
        "results_str = [item[0].page_content for item in results]\n",
        "for result in results_str:\n",
        "    print(result)\n",
        "    print('---------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "This key has been deactivated. I kept it in the notebook so you can see its formatting.\n",
        "https://platform.openai.com/account/billing/overview : Add to your credit balance and create an API key\n",
        "'''\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Create a LangChain retriever using the db to feed queried documents into a constructed prompt.\n",
        "'''\n",
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Create a template which takes in the queried docs as the context and the query to retrieve those docs as the question.\n",
        "\n",
        "The chain uses LangChain Expression Language (LCEL) to define how a question should be chained first to a dict where\n",
        "the docs are queried by the retriever, then to the prompt constructor, then to the llm, and then to the output parser.\n",
        "\n",
        "chain.invokes invokes that chain and returns the parsed output.\n",
        "'''\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "template = 'Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}'\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI(model_name=model_name, temperature=0)\n",
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()} \n",
        "    | prompt \n",
        "    | model \n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "question = 'What was the name of Benjamin Franklin\\'s newspaper?'\n",
        "chain.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "A final example where the query and the question are distinct.\n",
        "'''\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "template = 'Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}'\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI(model_name=model_name, temperature=0)\n",
        "\n",
        "question_dict = {\n",
        "    'question': '[Insert question here]', \n",
        "    'query': '[Insert query here]'\n",
        "    }\n",
        "question_dict['context'] = retriever.invoke(question_dict['query'])\n",
        "\n",
        "chain = (\n",
        "    prompt \n",
        "    | model \n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(question_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNyvtC67lt0rjksvsVD/bdI",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
